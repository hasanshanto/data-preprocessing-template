{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data Pre-processing Toolkits Template.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPlbhpOuj1hCS753lwZVN4z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Data Pre-processing Tools and Template** "],"metadata":{"id":"2_xq1I92eh-h"}},{"cell_type":"markdown","source":["##**Importing Necessary Libraries** "],"metadata":{"id":"JlPn9aGjexL6"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"AiE5bJwpeR6-","executionInfo":{"status":"ok","timestamp":1650385085021,"user_tz":-360,"elapsed":550,"user":{"displayName":"Md. Hasibul Hasan","userId":"11140975876796152310"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["## **Importing the Dataset**"],"metadata":{"id":"KZEOtMAQmTft"}},{"cell_type":"code","source":["dataset = pd.read_csv('Data.csv')\n","\n","X = dataset.iloc[:, :-1].values\n","y = dataset.iloc[:, -1].values"],"metadata":{"id":"t8ovpntnmlN-","executionInfo":{"status":"ok","timestamp":1650385085022,"user_tz":-360,"elapsed":9,"user":{"displayName":"Md. Hasibul Hasan","userId":"11140975876796152310"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## **Taking Care of Missing Data**"],"metadata":{"id":"vddIwQ2nmvqt"}},{"cell_type":"markdown","source":["Missing datas can be taking care by either ignoring, deleting or replacing the data using mean/ median strategy.\n","The imputer returns the index after replacing it with the mean of all data"],"metadata":{"id":"1kzRhqPVnktW"}},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer\n","\n","imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n","imputer.fit(X[:, 1:3])\n","X[:, 1:3] = imputer.transform(X[:, 1:3])"],"metadata":{"id":"RF3QVOwygC9m","executionInfo":{"status":"ok","timestamp":1650385085551,"user_tz":-360,"elapsed":537,"user":{"displayName":"Md. Hasibul Hasan","userId":"11140975876796152310"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## **Encoding Categorical Data**"],"metadata":{"id":"Foc67OCMm1H7"}},{"cell_type":"markdown","source":["If the data is continious (string) then we have to Encode the data into numbers so that one category can't dominate the other categor."],"metadata":{"id":"O54sE2u6oHPT"}},{"cell_type":"markdown","source":["### **Encoding Independent Data**"],"metadata":{"id":"FHvRFBuEm-fa"}},{"cell_type":"markdown","source":["The ColumnTransger method asks for several parameter\n","1. transformers ask for 3 parameter again\n","  1. what do you want to do? (Encoding)\n","  2. in which way? (OneHotEncoder)\n","  3. in which column? (0 in this case)\n","2. remainder ask about the other columns, what do you want to do with them, passthrough in this case."],"metadata":{"id":"9C-kEvlKodIE"}},{"cell_type":"code","source":["from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","\n","ct = ColumnTransformer(transformers= [('encoder', OneHotEncoder(), [0])], remainder= 'passthrough')\n","X = np.array(ct.fit_transform(X))"],"metadata":{"id":"2hMSskRbh0sE","executionInfo":{"status":"ok","timestamp":1650385085551,"user_tz":-360,"elapsed":38,"user":{"displayName":"Md. Hasibul Hasan","userId":"11140975876796152310"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### **Encoding Dependent Data**"],"metadata":{"id":"Pr_1VgWAm-eH"}},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","y = le.fit_transform(y)"],"metadata":{"id":"aq0QidPpikb8","executionInfo":{"status":"ok","timestamp":1650385085552,"user_tz":-360,"elapsed":37,"user":{"displayName":"Md. Hasibul Hasan","userId":"11140975876796152310"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"KAMuYzOgnkry"}},{"cell_type":"markdown","source":["## **Splitting the Data into Train and Test Data**"],"metadata":{"id":"1574PS_fnLSe"}},{"cell_type":"markdown","source":["Splitting the train test data into 80/20 is a kind of convention. \n","random_state is just a variable which fix the randomness of the splitting where I am using the famous '42', which answers all the questions of this universe. Just kidding!!"],"metadata":{"id":"Z-hckJRRpLiq"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 42)"],"metadata":{"id":"ilxoehIhjApc","executionInfo":{"status":"ok","timestamp":1650385085552,"user_tz":-360,"elapsed":36,"user":{"displayName":"Md. Hasibul Hasan","userId":"11140975876796152310"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## **Feature Scalling**"],"metadata":{"id":"_A10sgRxnLQ_"}},{"cell_type":"markdown","source":["we deploy feature scalling on our data to standarize the data into same scale. "],"metadata":{"id":"JDbGEBxXpt8H"}},{"cell_type":"markdown","source":["We use fit_transform() on the train data so that we learn the parameters of scaling on the train data and in the same time we scale the train data. We only use transform() on the test data because we use the scaling paramaters learned on the train data to scale the test data.\n","\n","This is the standart procedure to scale. You always learn your scaling parameters on the train and then use them on the test. "],"metadata":{"id":"GWgVpD3ppn98"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","sc = StandardScaler()\n","\n","X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n","X_test[:, 3:] = sc.transform(X_test[:, 3:])"],"metadata":{"id":"nRxvTQpajWHD","executionInfo":{"status":"ok","timestamp":1650385085553,"user_tz":-360,"elapsed":37,"user":{"displayName":"Md. Hasibul Hasan","userId":"11140975876796152310"}}},"execution_count":7,"outputs":[]}]}